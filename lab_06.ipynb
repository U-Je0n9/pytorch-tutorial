{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.FloatTensor ([1,2,3])\n",
    "hypothesis = F.softmax(z, dim=0)\n",
    "print(hypothesis)\n",
    "hypothesis.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2257, 0.1283, 0.2013, 0.1358, 0.3089],\n",
      "        [0.2552, 0.2790, 0.1620, 0.1508, 0.1530],\n",
      "        [0.2246, 0.1294, 0.2676, 0.2339, 0.1445]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z=torch.rand(3,5, requires_grad = True)\n",
    "hypothesis = F.softmax(z,dim=1)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "y = torch.randint(5,(3,)).long()\n",
    "print(y) # 답 정해주는 거 (인덱스)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_one_hot = torch.zeros_like(hypothesis)\n",
    "y_one_hot.scatter_(1, y.unsqueeze(1), 1) #첫 번째 1 -> dim=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4960, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with torch.nn.functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4884, -2.0537, -1.6031, -1.9964, -1.1747],\n",
      "        [-1.3657, -1.2766, -1.8200, -1.8919, -1.8774],\n",
      "        [-1.4934, -2.0452, -1.3181, -1.4529, -1.9343]], grad_fn=<LogBackward0>)\n",
      "tensor(1.4960, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.log(F.softmax(z,dim=1)))\n",
    "print((y_one_hot * -torch.log(F.softmax(z,dim=1))).sum(dim=1).mean())\n",
    "# Low level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4884, -2.0537, -1.6031, -1.9964, -1.1747],\n",
      "        [-1.3657, -1.2766, -1.8200, -1.8919, -1.8774],\n",
      "        [-1.4934, -2.0452, -1.3181, -1.4529, -1.9343]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor(1.4960, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.4960, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(F.log_softmax(z,dim=1))\n",
    "print(F.nll_loss(F.log_softmax(z,dim=1),y))  # Negative Log Likelihood\n",
    "print(F.cross_entropy(z,y))\n",
    "# High level (위랑 똑같은 값 출력)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Low-level Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.098612\n",
      "Epoch  100/1000 Cost: 0.975704\n",
      "Epoch  200/1000 Cost: 0.944895\n",
      "Epoch  300/1000 Cost: 0.928340\n",
      "Epoch  400/1000 Cost: 0.915387\n",
      "Epoch  500/1000 Cost: 0.904186\n",
      "Epoch  600/1000 Cost: 0.894344\n",
      "Epoch  700/1000 Cost: 0.885711\n",
      "Epoch  800/1000 Cost: 0.878166\n",
      "Epoch  900/1000 Cost: 0.871586\n",
      "Epoch 1000/1000 Cost: 0.865845\n"
     ]
    }
   ],
   "source": [
    "x_train = [[1,2,1,1],\n",
    "           [2,1,3,2],\n",
    "           [3,1,3,4],\n",
    "           [4,1,5,5],\n",
    "           [3,2,5,1],\n",
    "           [1,3,2,4],\n",
    "           [1,1,2,1],\n",
    "           [4,1,2,1]]\n",
    "           # x_train -> (8,4)\n",
    "y_train = [2,2,2,1,1,1,0,0]\n",
    "# y_train ->(8,)\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "\n",
    "w = torch.zeros((4,3), requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "optimizer = optim.SGD([w,b],lr = 0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1 ) :\n",
    "    hypothesis = F.softmax(x_train.matmul(w) + b , dim=1)\n",
    "    y_one_hot = torch.zeros_like (hypothesis)\n",
    "    y_one_hot.scatter_(1, y_train.unsqueeze(1), 1)\n",
    "    cost = (y_one_hot * -torch.log(F.softmax(hypothesis,dim=1))).sum(dim=1).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0 :\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 0.724171\n",
      "Epoch  100/1000 Cost: 0.724171\n",
      "Epoch  200/1000 Cost: 0.724171\n",
      "Epoch  300/1000 Cost: 0.724171\n",
      "Epoch  400/1000 Cost: 0.724171\n",
      "Epoch  500/1000 Cost: 0.724171\n",
      "Epoch  600/1000 Cost: 0.724171\n",
      "Epoch  700/1000 Cost: 0.724171\n",
      "Epoch  800/1000 Cost: 0.724171\n",
      "Epoch  900/1000 Cost: 0.724171\n",
      "Epoch 1000/1000 Cost: 0.724171\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1 ) :\n",
    "    z = x_train.matmul(w) + b\n",
    "    cost = F.cross_entropy(z,y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0 :\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()))\n",
    "\n",
    "# 왜 위쪽 코드는 여러번 실행해도 cost가 그대로인데 얘는 실행할수록 숫자가 점점 똑같아질까...?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level Implementation with nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.474953\n",
      "Epoch  100/1000 Cost: 0.707645\n",
      "Epoch  200/1000 Cost: 0.599398\n",
      "Epoch  300/1000 Cost: 0.528057\n",
      "Epoch  400/1000 Cost: 0.475295\n",
      "Epoch  500/1000 Cost: 0.434100\n",
      "Epoch  600/1000 Cost: 0.400780\n",
      "Epoch  700/1000 Cost: 0.373120\n",
      "Epoch  800/1000 Cost: 0.349687\n",
      "Epoch  900/1000 Cost: 0.329501\n",
      "Epoch 1000/1000 Cost: 0.311868\n"
     ]
    }
   ],
   "source": [
    "class SoftmaxClassifierModel (nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4,3)\n",
    "\n",
    "    def forward(self,x) :\n",
    "        return self.linear(x)\n",
    "\n",
    "model = SoftmaxClassifierModel()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters() ,lr = 0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1 ) :\n",
    "    prediction = model(x_train)\n",
    "    cost = F.cross_entropy(prediction, y_train)\n",
    "\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0 :\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()))\n",
    "    \n",
    "# 얘는 또 할 때마다 그냥 숫자가 다 바뀌는데 cost가 젤 적게 나오긴 하네..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2da23d7cea119a2496e177037895a769c276e25040c55c0d4dfaae947ee03ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
